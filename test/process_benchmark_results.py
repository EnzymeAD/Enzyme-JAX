# /// script
# requires-python = ">=3.12"
# dependencies = [
#     "absl-py",
#     "numpy",
#     "pandas",
#     "tabulate",
# ]
# ///

import collections
import glob
import os
import re
import tempfile
import zipfile
from datetime import datetime, timezone

import pandas as pd
from absl import app, flags, logging

_ARTIFACT_DIR = flags.DEFINE_string(
    "artifact_dir", None, "Path to the artifact directory", required=True
)

_MARKDOWN_FILE = flags.DEFINE_string(
    "markdown_file", None, "Path to the markdown file", required=True
)


def get_machine_name(filename: str) -> str:
    pattern = re.compile(r"benchmark-(.+?)-\d+\.\d+")
    match = pattern.search(filename)
    if match:
        return match.group(1)
    raise ValueError(f"Could not find machine name in filename: {filename}")


def find_all_zip_files(artifact_dir: str) -> list[str]:
    results = []
    for file in glob.glob(f"{artifact_dir}/**/*.zip", recursive=True):
        filename = os.path.basename(file)
        if filename.startswith("Enzyme-JAX-benchmark-"):
            logging.info("Found zip file: %s", file)
            results.append(file)
    return results


def main(_) -> None:
    all_zip_files = find_all_zip_files(_ARTIFACT_DIR.value)

    # This will extract the bazel-testlogs directory to a temporary directory
    temp_dir = tempfile.TemporaryDirectory(delete=True)
    logging.info("Extracting test logs to %s", temp_dir.name)
    for zip_file in all_zip_files:
        machine_name = get_machine_name(zip_file)
        logging.info("Processing %s...", zip_file)

        with zipfile.ZipFile(zip_file, "r") as zip_ref:
            zip_ref.extractall(os.path.join(temp_dir.name, machine_name))

    # Now we need to unzip the outputs.zip
    tempdir_results = tempfile.TemporaryDirectory(delete=False)
    logging.info("Extracting outputs to %s", tempdir_results.name)

    for file in glob.glob(f"{temp_dir.name}/**/outputs.zip", recursive=True):
        logging.info("Unzipping %s...", file)
        machine_name = file.strip(f"{temp_dir.name}/").split("/")[0]

        with zipfile.ZipFile(file, "r") as zip_ref:
            zip_ref.extractall(os.path.join(tempdir_results.name, machine_name))

    # Merge the CSVs into a single dataframe
    combined_dfs = collections.defaultdict(list)
    for file in glob.glob(f"{tempdir_results.name}/*/results_*.csv"):
        machine_name = os.path.basename(os.path.dirname(file))
        expt = os.path.basename(file).replace("results_", "").replace(".csv", "")

        df = pd.read_csv(file)
        df["machine"] = machine_name

        combined_dfs[expt].append(df)

    # Merge the CSVs into a single dataframe
    final_dfs = dict()
    for expt, dfs in combined_dfs.items():
        merged = pd.concat(dfs, ignore_index=True)
        merged = merged.drop("Benchmark Name", axis=1)
        final_dfs[expt] = merged

    # Create a markdown table
    with open(_MARKDOWN_FILE.value, "w") as f:
        f.write("## ðŸš€ Benchmark Results\n\n")

        for expt, df in final_dfs.items():
            f.write(f"<details>\n<summary><strong>ðŸ“Š {expt}</strong></summary>\n\n")
            df.to_markdown(f, index=False)
            f.write("\n\n</details>\n\n")

        f.write(
            f"_Generated by GitHub Actions on {datetime.now(timezone.utc).isoformat()}_\n"
        )


if __name__ == "__main__":
    app.run(main)()
