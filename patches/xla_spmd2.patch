diff --git a/xla/service/spmd/spmd_partitioner.cc b/xla/service/spmd/spmd_partitioner.cc
index 54fdd9a0fe..d0fde9b443 100644
--- a/xla/service/spmd/spmd_partitioner.cc
+++ b/xla/service/spmd/spmd_partitioner.cc
@@ -570,6 +570,17 @@ PartitionedHlo PartitionedHlo::ReshardNoCache(
     return PartitionedHlo(partitioned, base_shape_, state_);
   }
 
+  if (state_.module->config().debug_options().xla_enable_enzyme_comms_opt()) {
+    if (hlo_->opcode() == HloOpcode::kBroadcast && hlo_->operand(0)->shape().dimensions().size() == 0 &&
+       hlo_->operand(0)->IsConstant()) {
+      HloInstruction* new_broadcast =
+          state_.b->AddInstruction(HloInstruction::CreateBroadcast(
+              hlo_->shape(), hlo_->mutable_operand(0), {}));
+      new_broadcast->set_sharding(target);
+      return PartitionedHlo(new_broadcast, base_shape_, state_);
+    }
+  }
+
   if (CanReshardWithCollectivePermute(sharding(), target)) {
     return ReshardWithCollectivePermute(target);
   }
@@ -3879,8 +3890,8 @@ SpmdPartitioningVisitor::HandleDUSAllPartitionedSliceDimsHaveConstantIndices(
   const Shape sharded_update_pred_shape =
       MakePartitionedShape(update_pred_shape, hlo->sharding());
 
-  auto zeroOperand = CreateZero(sharded_update_pred_shape, &b_);
-  zeroOperand->set_sharding(hlo->sharding());
+  auto zeroOperand = CreateZero(update_pred_shape, &b_);
+  zeroOperand->set_sharding(HloSharding::Replicate());
 
   HloInstruction* paddingValue = CreateOne(Shape(PRED, {}), &b_);
   HloInstruction* maskOp = PadHelper(
